{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "280e3f2c-2f27-45c3-8c3b-3c7519dad440",
   "metadata": {},
   "source": [
    "## Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "191ec918-cc0f-43bf-8200-86f56c6e57f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image \n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import (Dense,\n",
    "                                     Dropout,\n",
    "                                     Activation,\n",
    "                                     Conv2D,\n",
    "                                     MaxPooling2D,\n",
    "                                     BatchNormalization,\n",
    "                                     Flatten)\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.callbacks import (EarlyStopping, \n",
    "                                        ReduceLROnPlateau, \n",
    "                                        ModelCheckpoint)\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180bf005-8ce5-4860-9306-336afc0fc57a",
   "metadata": {},
   "source": [
    "## Reading all images and storing them in data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7461ea72-fdfd-4b91-ab6f-e7d89da3515a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "happy\n",
      "sad\n",
      "fear\n",
      "surprise\n",
      "neutral\n",
      "angry\n",
      "disgust\n",
      "train\n",
      "happy\n",
      "sad\n",
      "fear\n",
      "surprise\n",
      "neutral\n",
      "angry\n",
      "disgust\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>images</th>\n",
       "      <th>labels</th>\n",
       "      <th>purpose</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[[5], [4], [5], [9], [10], [9], [10], [12], [...</td>\n",
       "      <td>happy</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[[19], [21], [22], [18], [20], [21], [16], [1...</td>\n",
       "      <td>happy</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[[228], [229], [230], [229], [228], [227], [2...</td>\n",
       "      <td>happy</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[[25], [33], [43], [30], [46], [84], [105], [...</td>\n",
       "      <td>happy</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[[33], [29], [15], [15], [20], [36], [40], [5...</td>\n",
       "      <td>happy</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              images labels purpose\n",
       "0  [[[5], [4], [5], [9], [10], [9], [10], [12], [...  happy       V\n",
       "1  [[[19], [21], [22], [18], [20], [21], [16], [1...  happy       V\n",
       "2  [[[228], [229], [230], [229], [228], [227], [2...  happy       V\n",
       "3  [[[25], [33], [43], [30], [46], [84], [105], [...  happy       V\n",
       "4  [[[33], [29], [15], [15], [20], [36], [40], [5...  happy       V"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int2emotions = {0:'angry',\n",
    "                1:'fear',\n",
    "                2:'happy',\n",
    "                3:'neutral',\n",
    "                4:'sad',\n",
    "                5:'surprise'}\n",
    "emotions2int = {'angry':0,\n",
    "                'fear':1,\n",
    "                'happy':2,\n",
    "                'neutral':3,\n",
    "                'sad':4,\n",
    "                'surprise':5}\n",
    "\n",
    "dic = {'images':[],\n",
    "       'labels':[],\n",
    "       'purpose':[]}\n",
    "\n",
    "for d in os.listdir('/Users/Chabi/Documents/Python_projects/Emotions/data/'):\n",
    "    if not d.startswith('.'):\n",
    "        print(d)\n",
    "        for emotion in os.listdir(f'/Users/Chabi/Documents/Python_projects/Emotions/data/{d}'):\n",
    "            if not emotion.startswith('.'):\n",
    "                print(emotion)\n",
    "                for i in os.listdir(f'/Users/Chabi/Documents/Python_projects/Emotions/data/{d}/{emotion}'):\n",
    "                    img = cv2.imread(f'/Users/Chabi/Documents/Python_projects/Emotions/data/{d}/{emotion}/{i}', 0)\n",
    "                    img = img.reshape(48, 48, 1)\n",
    "\n",
    "                    dic['images'].append(img)\n",
    "                    dic['labels'].append(emotion)\n",
    "\n",
    "                    if d=='train':\n",
    "                        dic['purpose'].append('T')\n",
    "                    else:\n",
    "                        dic['purpose'].append('V')\n",
    "\n",
    "df = pd.DataFrame(dic)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c93ecc5a-cb1d-4e08-9bc4-347875d08a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = df[df['purpose'] == 'T']\n",
    "val_data = df[df['purpose'] == 'V']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2704f653-f6f1-49b7-9e0f-68ec5cebc426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>images</th>\n",
       "      <th>labels</th>\n",
       "      <th>purpose</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7178</th>\n",
       "      <td>[[[108], [83], [63], [65], [89], [111], [121],...</td>\n",
       "      <td>happy</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7179</th>\n",
       "      <td>[[[137], [142], [159], [162], [158], [134], [1...</td>\n",
       "      <td>happy</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7180</th>\n",
       "      <td>[[[111], [148], [155], [167], [181], [191], [1...</td>\n",
       "      <td>happy</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7181</th>\n",
       "      <td>[[[151], [156], [121], [100], [80], [116], [15...</td>\n",
       "      <td>happy</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7182</th>\n",
       "      <td>[[[248], [187], [149], [130], [97], [140], [13...</td>\n",
       "      <td>happy</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 images labels purpose\n",
       "7178  [[[108], [83], [63], [65], [89], [111], [121],...  happy       T\n",
       "7179  [[[137], [142], [159], [162], [158], [134], [1...  happy       T\n",
       "7180  [[[111], [148], [155], [167], [181], [191], [1...  happy       T\n",
       "7181  [[[151], [156], [121], [100], [80], [116], [15...  happy       T\n",
       "7182  [[[248], [187], [149], [130], [97], [140], [13...  happy       T"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cf6db11-b71e-400c-8165-8eca255af5f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>images</th>\n",
       "      <th>labels</th>\n",
       "      <th>purpose</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[[5], [4], [5], [9], [10], [9], [10], [12], [...</td>\n",
       "      <td>happy</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[[19], [21], [22], [18], [20], [21], [16], [1...</td>\n",
       "      <td>happy</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[[228], [229], [230], [229], [228], [227], [2...</td>\n",
       "      <td>happy</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[[25], [33], [43], [30], [46], [84], [105], [...</td>\n",
       "      <td>happy</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[[33], [29], [15], [15], [20], [36], [40], [5...</td>\n",
       "      <td>happy</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              images labels purpose\n",
       "0  [[[5], [4], [5], [9], [10], [9], [10], [12], [...  happy       V\n",
       "1  [[[19], [21], [22], [18], [20], [21], [16], [1...  happy       V\n",
       "2  [[[228], [229], [230], [229], [228], [227], [2...  happy       V\n",
       "3  [[[25], [33], [43], [30], [46], [84], [105], [...  happy       V\n",
       "4  [[[33], [29], [15], [15], [20], [36], [40], [5...  happy       V"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2980e8c-2462-403c-8190-8fcd6a15c48f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "happy       7215\n",
       "neutral     4965\n",
       "sad         4830\n",
       "fear        4097\n",
       "angry       3995\n",
       "surprise    3171\n",
       "disgust      436\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eca8eccc-b974-4ce4-8785-b4c1bfd1ab7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "happy       1774\n",
       "sad         1247\n",
       "neutral     1233\n",
       "fear        1024\n",
       "angry        958\n",
       "surprise     831\n",
       "disgust      111\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe305762-0ca1-42f7-bc24-201302d24bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28709\n",
      "7178\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "print(len(val_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b092947-37db-408b-93f0-036c784f1737",
   "metadata": {},
   "source": [
    "## Taking equal instances of all classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef8fe6d8-64f2-40ad-b003-6f2d19ace4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_df = train_data[train_data['labels'] == 'happy'].sample(n=3171)\n",
    "neutral_df = train_data[train_data['labels'] == 'neutral'].sample(n=3171)\n",
    "sad_df = train_data[train_data['labels'] == 'sad'].sample(n=3171)\n",
    "fear_df = train_data[train_data['labels'] == 'fear'].sample(n=3171)\n",
    "angry_df = train_data[train_data['labels'] == 'angry'].sample(n=3171)\n",
    "suprise_df = train_data[train_data['labels'] == 'surprise'].sample(n=3171)\n",
    "\n",
    "train_data = pd.concat([happy_df,\n",
    "                       neutral_df,\n",
    "                       sad_df,\n",
    "                       fear_df,\n",
    "                       angry_df,\n",
    "                       suprise_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "836e5932-7e6d-4c01-9c15-6ff0d31d92a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = val_data[val_data['labels'] != 'disgust']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9bf0adf-e44d-4c9e-a7d6-12db7f78f86a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "happy       1774\n",
       "sad         1247\n",
       "neutral     1233\n",
       "fear        1024\n",
       "angry        958\n",
       "surprise     831\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eccc89a2-de18-4b4e-8494-5d7006db91ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data = train_data.sample(frac=1)\n",
    "train_data.reset_index(inplace=True)\n",
    "train_data.drop('index', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ac13d3b-e16e-47da-b2fb-ecce1cabe714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>images</th>\n",
       "      <th>labels</th>\n",
       "      <th>purpose</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[[249], [255], [164], [73], [43], [50], [68],...</td>\n",
       "      <td>angry</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[[21], [20], [25], [34], [42], [19], [13], [1...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[[38], [39], [76], [88], [96], [115], [123], ...</td>\n",
       "      <td>surprise</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[[203], [198], [199], [199], [200], [195], [2...</td>\n",
       "      <td>fear</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[[161], [159], [184], [183], [182], [185], [1...</td>\n",
       "      <td>angry</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              images    labels purpose\n",
       "0  [[[249], [255], [164], [73], [43], [50], [68],...     angry       T\n",
       "1  [[[21], [20], [25], [34], [42], [19], [13], [1...   neutral       T\n",
       "2  [[[38], [39], [76], [88], [96], [115], [123], ...  surprise       T\n",
       "3  [[[203], [198], [199], [199], [200], [195], [2...      fear       T\n",
       "4  [[[161], [159], [184], [183], [182], [185], [1...     angry       T"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ea8f7fd-a113-424b-8847-359a5850cdc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "angry       3171\n",
       "neutral     3171\n",
       "surprise    3171\n",
       "fear        3171\n",
       "happy       3171\n",
       "sad         3171\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e71fa04-0e52-4684-9c14-ae216a2d2861",
   "metadata": {},
   "source": [
    "## Declaring some constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13b048fe-d62f-4f89-aba4-0ff763cc92c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32 \n",
    "classes = 6\n",
    "rows, columns = 48, 48"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a514622-24e6-455a-8d7f-e6745eda3f1c",
   "metadata": {},
   "source": [
    "## Getting data for the Emotion Detector model in the right shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8a5769f-29bd-4e4d-9767-f7b634ea76d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = list(train_data['labels'].replace(emotions2int))\n",
    "train_labels = to_categorical(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e29d70b0-94fc-4898-a270-4ea67bd2b01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_labels = list(val_data['labels'].replace(emotions2int))\n",
    "val_labels = to_categorical(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d299f724-bb34-432a-9059-d20ff5d6b7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = list(train_data['images'])\n",
    "train_data = np.array(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16cd63af-3293-498a-8cda-737efffc56c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = list(val_data['images'])\n",
    "val_data = np.array(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "676b5e1f-7685-4d93-96e5-f80063176ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19026, 48, 48, 1)\n",
      "(7067, 48, 48, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(val_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c652c2e-86eb-4610-95ae-e441168b1219",
   "metadata": {},
   "source": [
    "## Creating the Emotion Detector model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b22e503-e3e7-406e-b198-860d0e8fd6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 48, 48, 64)        640       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 48, 48, 64)       256       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 48, 48, 64)        36928     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 48, 48, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 24, 24, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 24, 24, 64)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 24, 24, 128)       73856     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 24, 24, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 24, 24, 128)       147584    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 24, 24, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 12, 12, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 12, 12, 128)       0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 12, 12, 256)       295168    \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 12, 12, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 12, 12, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 12, 12, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 6, 6, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 6, 6, 256)         0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 6, 6, 512)         1180160   \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 6, 6, 512)        2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 6, 6, 512)         2359808   \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 6, 6, 512)        2048      \n",
      " hNormalization)                                                 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-13 16:59:18.126376: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 3, 3, 512)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 3, 3, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4608)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               1179904   \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 256)              1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 128)              512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_10 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 6)                 390       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,915,142\n",
      "Trainable params: 5,910,406\n",
      "Non-trainable params: 4,736\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# First Block\n",
    "\n",
    "model.add(Conv2D(64,\n",
    "                 (3,3),\n",
    "                 activation='elu',\n",
    "                 input_shape=(rows,columns,1),\n",
    "                 kernel_initializer='he_normal',\n",
    "                 padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64,\n",
    "                 (3,3),\n",
    "                 activation='elu',\n",
    "                 input_shape=(rows,columns,1),\n",
    "                 kernel_initializer='he_normal',\n",
    "                 padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Second Block\n",
    "\n",
    "model.add(Conv2D(128,\n",
    "                 (3,3),\n",
    "                 activation='elu',\n",
    "                 kernel_initializer='he_normal',\n",
    "                 padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128,\n",
    "                 (3,3),\n",
    "                 activation='elu',\n",
    "                 kernel_initializer='he_normal',\n",
    "                 padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Third Block\n",
    "\n",
    "model.add(Conv2D(256,\n",
    "                 (3,3),\n",
    "                 activation='elu',\n",
    "                 kernel_initializer='he_normal',\n",
    "                 padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(256,\n",
    "                 (3,3),\n",
    "                 activation='elu',\n",
    "                 kernel_initializer='he_normal',\n",
    "                 padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Fourth Block\n",
    "\n",
    "model.add(Conv2D(512,\n",
    "                 (3,3),\n",
    "                 activation='elu',\n",
    "                 kernel_initializer='he_normal',\n",
    "                 padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(512,(3,3),\n",
    "                 activation='elu',\n",
    "                 kernel_initializer='he_normal',\n",
    "                 padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Fifth Block\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256,\n",
    "                activation='elu',\n",
    "                kernel_initializer='he_normal'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Sixth Block\n",
    "\n",
    "model.add(Dense(128,\n",
    "                activation='elu',\n",
    "                kernel_initializer='he_normal'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Seventh Block\n",
    "\n",
    "model.add(Dense(64,\n",
    "                activation='elu',\n",
    "                kernel_initializer='he_normal'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Eighth Block\n",
    "\n",
    "model.add(Dense(classes,\n",
    "                activation='softmax',\n",
    "                kernel_initializer='he_normal'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54aeaf4c-fda7-45c0-9207-d3c49402912a",
   "metadata": {},
   "source": [
    "## Declaring callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "264435b2-2455-4405-ac91-950d78c05ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('model/6_class_emotion_detector_V2.h5',\n",
    "                             save_best_only=True,\n",
    "                             mode='min',\n",
    "                             monitor='val_loss',\n",
    "                             verbose=1)\n",
    "\n",
    "earlystopping = EarlyStopping(patience=10,\n",
    "                              verbose=1,\n",
    "                              min_delta=0,\n",
    "                              monitor='val_loss',\n",
    "                              restore_best_weights=True)\n",
    "\n",
    "callbacks = [checkpoint, earlystopping]\n",
    "\n",
    "model.compile(metrics=['accuracy'],\n",
    "              optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ffe798-dc9f-417f-9843-86d4e6bfe764",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d8ee9969-6dc7-4e60-bc57-27e4e421ab96",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = 28273\n",
    "validation_samples = 3534\n",
    "batch_size = 64\n",
    "eopchs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8193d896-290a-49a9-af0c-cf789847c50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "441/441 [==============================] - ETA: 0s - loss: 2.1485 - accuracy: 0.2137\n",
      "Epoch 1: val_loss improved from inf to 2.07411, saving model to model/6_class_emotion_detector_V2.h5\n",
      "441/441 [==============================] - 320s 723ms/step - loss: 2.1485 - accuracy: 0.2137 - val_loss: 2.0741 - val_accuracy: 0.2329\n",
      "Epoch 2/30\n",
      "441/441 [==============================] - ETA: 0s - loss: 1.6155 - accuracy: 0.3311\n",
      "Epoch 2: val_loss improved from 2.07411 to 1.44666, saving model to model/6_class_emotion_detector_V2.h5\n",
      "441/441 [==============================] - 325s 738ms/step - loss: 1.6155 - accuracy: 0.3311 - val_loss: 1.4467 - val_accuracy: 0.4324\n",
      "Epoch 3/30\n",
      "441/441 [==============================] - ETA: 0s - loss: 1.4051 - accuracy: 0.4381\n",
      "Epoch 3: val_loss improved from 1.44666 to 1.28461, saving model to model/6_class_emotion_detector_V2.h5\n",
      "441/441 [==============================] - 313s 710ms/step - loss: 1.4051 - accuracy: 0.4381 - val_loss: 1.2846 - val_accuracy: 0.4996\n",
      "Epoch 4/30\n",
      "441/441 [==============================] - ETA: 0s - loss: 1.2845 - accuracy: 0.5009\n",
      "Epoch 4: val_loss improved from 1.28461 to 1.17866, saving model to model/6_class_emotion_detector_V2.h5\n",
      "441/441 [==============================] - 317s 719ms/step - loss: 1.2845 - accuracy: 0.5009 - val_loss: 1.1787 - val_accuracy: 0.5441\n",
      "Epoch 5/30\n",
      "441/441 [==============================] - ETA: 0s - loss: 1.2096 - accuracy: 0.5379\n",
      "Epoch 5: val_loss did not improve from 1.17866\n",
      "441/441 [==============================] - 315s 713ms/step - loss: 1.2096 - accuracy: 0.5379 - val_loss: 1.1823 - val_accuracy: 0.5434\n",
      "Epoch 6/30\n",
      "441/441 [==============================] - ETA: 0s - loss: 1.1331 - accuracy: 0.5721\n",
      "Epoch 6: val_loss improved from 1.17866 to 1.12759, saving model to model/6_class_emotion_detector_V2.h5\n",
      "441/441 [==============================] - 317s 719ms/step - loss: 1.1331 - accuracy: 0.5721 - val_loss: 1.1276 - val_accuracy: 0.5649\n",
      "Epoch 7/30\n",
      "441/441 [==============================] - ETA: 0s - loss: 1.0662 - accuracy: 0.6021\n",
      "Epoch 7: val_loss did not improve from 1.12759\n",
      "441/441 [==============================] - 313s 711ms/step - loss: 1.0662 - accuracy: 0.6021 - val_loss: 1.1650 - val_accuracy: 0.5547\n",
      "Epoch 8/30\n",
      "441/441 [==============================] - ETA: 0s - loss: 0.9908 - accuracy: 0.6374\n",
      "Epoch 8: val_loss did not improve from 1.12759\n",
      "441/441 [==============================] - 315s 714ms/step - loss: 0.9908 - accuracy: 0.6374 - val_loss: 1.1693 - val_accuracy: 0.5578\n",
      "Epoch 9/30\n",
      "441/441 [==============================] - ETA: 0s - loss: 0.9059 - accuracy: 0.6720\n",
      "Epoch 9: val_loss improved from 1.12759 to 1.11448, saving model to model/6_class_emotion_detector_V2.h5\n",
      "441/441 [==============================] - 317s 718ms/step - loss: 0.9059 - accuracy: 0.6720 - val_loss: 1.1145 - val_accuracy: 0.6053\n",
      "Epoch 10/30\n",
      "441/441 [==============================] - ETA: 0s - loss: 0.8148 - accuracy: 0.7131\n",
      "Epoch 10: val_loss did not improve from 1.11448\n",
      "441/441 [==============================] - 315s 715ms/step - loss: 0.8148 - accuracy: 0.7131 - val_loss: 1.1543 - val_accuracy: 0.6051\n",
      "Epoch 11/30\n",
      "441/441 [==============================] - ETA: 0s - loss: 0.7279 - accuracy: 0.7472\n",
      "Epoch 11: val_loss did not improve from 1.11448\n",
      "441/441 [==============================] - 316s 716ms/step - loss: 0.7279 - accuracy: 0.7472 - val_loss: 1.2261 - val_accuracy: 0.5953\n",
      "Epoch 12/30\n",
      "441/441 [==============================] - ETA: 0s - loss: 0.6471 - accuracy: 0.7786\n",
      "Epoch 12: val_loss did not improve from 1.11448\n",
      "441/441 [==============================] - 315s 714ms/step - loss: 0.6471 - accuracy: 0.7786 - val_loss: 1.3345 - val_accuracy: 0.5895\n",
      "Epoch 13/30\n",
      "441/441 [==============================] - ETA: 0s - loss: 0.5624 - accuracy: 0.8127\n",
      "Epoch 13: val_loss did not improve from 1.11448\n",
      "441/441 [==============================] - 315s 715ms/step - loss: 0.5624 - accuracy: 0.8127 - val_loss: 1.3649 - val_accuracy: 0.6072\n",
      "Epoch 14/30\n",
      "441/441 [==============================] - ETA: 0s - loss: 0.4971 - accuracy: 0.8352\n",
      "Epoch 14: val_loss did not improve from 1.11448\n",
      "441/441 [==============================] - 315s 714ms/step - loss: 0.4971 - accuracy: 0.8352 - val_loss: 1.3747 - val_accuracy: 0.6079\n",
      "Epoch 15/30\n",
      "441/441 [==============================] - ETA: 0s - loss: 0.4305 - accuracy: 0.8600\n",
      "Epoch 15: val_loss did not improve from 1.11448\n",
      "441/441 [==============================] - 315s 715ms/step - loss: 0.4305 - accuracy: 0.8600 - val_loss: 1.5161 - val_accuracy: 0.6000\n",
      "Epoch 16/30\n",
      "441/441 [==============================] - ETA: 0s - loss: 0.3820 - accuracy: 0.8792\n",
      "Epoch 16: val_loss did not improve from 1.11448\n",
      "441/441 [==============================] - 316s 716ms/step - loss: 0.3820 - accuracy: 0.8792 - val_loss: 1.5917 - val_accuracy: 0.5902\n",
      "Epoch 17/30\n",
      "441/441 [==============================] - ETA: 0s - loss: 0.3462 - accuracy: 0.8922\n",
      "Epoch 17: val_loss did not improve from 1.11448\n",
      "441/441 [==============================] - 315s 714ms/step - loss: 0.3462 - accuracy: 0.8922 - val_loss: 1.6129 - val_accuracy: 0.5988\n",
      "Epoch 18/30\n",
      "441/441 [==============================] - ETA: 0s - loss: 0.3103 - accuracy: 0.9014\n",
      "Epoch 18: val_loss did not improve from 1.11448\n",
      "441/441 [==============================] - 315s 715ms/step - loss: 0.3103 - accuracy: 0.9014 - val_loss: 1.6486 - val_accuracy: 0.5974\n",
      "Epoch 19/30\n",
      "441/441 [==============================] - ETA: 0s - loss: 0.2830 - accuracy: 0.9094\n",
      "Epoch 19: val_loss did not improve from 1.11448\n",
      "Restoring model weights from the end of the best epoch: 9.\n",
      "441/441 [==============================] - 316s 716ms/step - loss: 0.2830 - accuracy: 0.9094 - val_loss: 1.8138 - val_accuracy: 0.5828\n",
      "Epoch 19: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_data,\n",
    "                    train_labels,\n",
    "                    epochs=eopchs,\n",
    "                    steps_per_epoch=train_samples//batch_size,\n",
    "                    validation_data=(val_data, val_labels),\n",
    "                    validation_steps=validation_samples//batch_size,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf5f64c-c7c1-4491-94d5-e4a5a5f5bc94",
   "metadata": {},
   "source": [
    "## Live Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "460edbf8-6706-4c7f-9cce-40db92fcee93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = load_model('model/6_class_emotion_detector_V2.h5')\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "classifier = cv2.CascadeClassifier('Haarcascades/haarcascade_frontalface_default.xml')\n",
    "\n",
    "def detect_face(frame):\n",
    "    faces=classifier.detectMultiScale(frame,1.3,4)\n",
    "    if faces==():\n",
    "        return frame\n",
    "    for x,y,w,h in faces:\n",
    "        cv2.rectangle(frame, \n",
    "                      (x,y), \n",
    "                      (x+w, y+h), \n",
    "                      (172, 42, 251),\n",
    "                      2)\n",
    "        face = frame[y:y+h, x:x+w]\n",
    "        face = cv2.cvtColor(face,\n",
    "                            cv2.COLOR_BGR2GRAY)\n",
    "        face = cv2.resize(face, \n",
    "                          (48, 48))\n",
    "        face = face.reshape(1, 48, 48, 1)\n",
    "        cv2.putText(frame, \n",
    "                    text=int2emotions[np.argmax(model.predict(face))],\n",
    "                    org=(x,y-15),\n",
    "                    fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    fontScale=1,\n",
    "                    color=(106, 40, 243),\n",
    "                    thickness=2)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af14145-e800-4560-a4c2-995237342192",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 166ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/78/p_f9j60x7y35v_tvsjrt49xw0000gn/T/ipykernel_21252/1868756630.py:8: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if faces==():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n"
     ]
    }
   ],
   "source": [
    "while 1:\n",
    "    ret, frame = cap.read()\n",
    "    if ret == True:\n",
    "        cv2.imshow('emotion_detector', detect_face(frame))\n",
    "        if cv2.waitKey(1) == 27:\n",
    "            break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c25d81-6d8c-4919-b0ad-e69ff7d0c087",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emotions",
   "language": "python",
   "name": "emotions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
