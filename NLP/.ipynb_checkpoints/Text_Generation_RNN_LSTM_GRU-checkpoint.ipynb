{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d177a193-6ec0-4753-8b2a-09b675f97b76",
   "metadata": {},
   "source": [
    "# Deep Learning - Text Generation using RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb67a292-084c-4f78-b808-cd13affb602e",
   "metadata": {},
   "source": [
    "## Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "556b08a3-2f78-45d9-85c5-6090c093f9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout, LSTM, GRU\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow.keras.utils as ku \n",
    "import tensorflow\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import time\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7744a8-38d0-420b-8fec-b73639e47149",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52965b8b-49aa-4fe0-9507-aa2814425e99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1250"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_headlines = []\n",
    "article_df = pd.read_csv('ArticlesMarch2018.csv')\n",
    "all_headlines.extend(list(article_df.headline.values))\n",
    "all_headlines = [h for h in all_headlines if h != 'Unknown']\n",
    "len(all_headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a0a09fd-f8d2-45e0-9274-df1f702da62a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Virtual Coins, Real Resources',\n",
       " 'U.S. Advances Military Plans for North Korea',\n",
       " 'Mr. Trump and the ‘Very Bad Judge’',\n",
       " 'To Erase Dissent, China Bans Pooh Bear and ‘N’',\n",
       " 'Loans Flowed to Kushner Cos. After Visits to the White House',\n",
       " 'China Envoy Intends To Ease Trade Tensions',\n",
       " 'President Trump’s Contradictory, and Sometimes False, Comments About Gun Policy to Lawmakers',\n",
       " 'Classic Letter Puzzle',\n",
       " 'Silicon Valley Disruption In an Australian School',\n",
       " '‘The Assassination of Gianni Versace’ Episode 6: A Nothing Man']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_headlines[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cdfb27-65ec-47db-aa4b-0a136c85fe80",
   "metadata": {},
   "source": [
    "## Dataset preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7297f4e9-fd86-4a3f-9792-67a129f5e06e",
   "metadata": {},
   "source": [
    "### Dataset cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ab8f10a-ec60-4d56-a0b6-24f99232c0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(txt):\n",
    "    txt = ''.join(v for v in txt if v not in string.punctuation).lower()\n",
    "    txt = txt.encode('utf8').decode('ascii', 'ignore')\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65793678-a317-45ca-9bab-cd817d05f380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['virtual coins real resources',\n",
       " 'us advances military plans for north korea',\n",
       " 'mr trump and the very bad judge',\n",
       " 'to erase dissent china bans pooh bear and n',\n",
       " 'loans flowed to kushner cos after visits to the white house',\n",
       " 'china envoy intends to ease trade tensions',\n",
       " 'president trumps contradictory and sometimes false comments about gun policy to lawmakers',\n",
       " 'classic letter puzzle',\n",
       " 'silicon valley disruption in an australian school',\n",
       " 'the assassination of gianni versace episode 6 a nothing man']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [clean_text(x) for x in all_headlines]\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a140983-88c6-4a86-90ae-e3eebb5ebc1d",
   "metadata": {},
   "source": [
    "### Generating Sequence of N-gram Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fbc7fe72-4a75-4d1d-b7c3-cba2909eab6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33, 469, 1062, 3, 191, 16, 84, 767]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "token_list = tokenizer.texts_to_sequences(['I am happy to see you here today'])[0]\n",
    "print(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc02bf6f-ab9f-4567-8786-d448016e4d4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[33, 469],\n",
       " [33, 469, 1062],\n",
       " [33, 469, 1062, 3],\n",
       " [33, 469, 1062, 3, 191],\n",
       " [33, 469, 1062, 3, 191, 16],\n",
       " [33, 469, 1062, 3, 191, 16, 84],\n",
       " [33, 469, 1062, 3, 191, 16, 84, 767]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check=[]\n",
    "\n",
    "for i in range(1, len(token_list)):\n",
    "    n_gram_sequence = token_list[:i+1]\n",
    "    check.append(n_gram_sequence)\n",
    "    \n",
    "check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0771fa7-89a8-481a-910c-6e8d9362f803",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "def get_sequence_of_tokens(corpus):\n",
    "    # tokenization\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "    \n",
    "    # convert data to sequence of tokens \n",
    "    input_sequences = []\n",
    "    for line in corpus:\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "    return input_sequences, total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a90d8e0c-b1df-459e-97a3-cdaf341404e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_sequences, total_words = get_sequence_of_tokens(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "78d9655e-6c7f-40dc-9e62-7a69e172906a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1119, 1120],\n",
       " [1119, 1120, 116],\n",
       " [1119, 1120, 116, 1121],\n",
       " [31, 1122],\n",
       " [31, 1122, 589],\n",
       " [31, 1122, 589, 392],\n",
       " [31, 1122, 589, 392, 7],\n",
       " [31, 1122, 589, 392, 7, 61],\n",
       " [31, 1122, 589, 392, 7, 61, 70],\n",
       " [117, 10]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_sequences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ce4f4f52-1891-44f5-8157-7013ed8ff7ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3582"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c101d8-c06f-4f46-b0b0-d6dc9d4316bc",
   "metadata": {},
   "source": [
    "### Padding the Sequences and obtain Variables : Predictors and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e3f63c14-65cf-4093-86bb-c8fedfaf3f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_padded_sequences(input_sequences):\n",
    "    max_sequence_len = max([len(x) for x in input_sequences])\n",
    "    input_sequences = np.array(pad_sequences(input_sequences,\n",
    "                                             maxlen=max_sequence_len,\n",
    "                                             padding='pre'))\n",
    "    \n",
    "    predictors, label = input_sequences[:, :-1], input_sequences[:, -1]\n",
    "    label = ku.to_categorical(label, num_classes=total_words)\n",
    "    return predictors, label, max_sequence_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "40d00e98-1965-4948-9987-5e489ff4681e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors, label, max_sequence_len = generate_padded_sequences(inp_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f88d91f0-644f-4773-ac53-14ed0684fa30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(threshold=np.inf)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d0c72e07-2394-49fa-a051-3f64e97b7195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ...    0    0 1119]\n",
      " [   0    0    0 ...    0 1119 1120]\n",
      " [   0    0    0 ... 1119 1120  116]\n",
      " ...\n",
      " [   0    0    0 ...  979  151  386]\n",
      " [   0    0    0 ...    0    0 3581]\n",
      " [   0    0    0 ...    0 3581    5]]\n"
     ]
    }
   ],
   "source": [
    "print(predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "735357f5-bfad-47cf-8dfe-fc1c4fb3837a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3c12f859-2ada-441c-b1b9-a20614dc90a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3582"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label[0]) # total words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3e6c5979-241e-43e8-b173-3083f299536a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sequence_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0ccc6a-5eb5-4c47-af63-9324e123b8e6",
   "metadata": {},
   "source": [
    "## RNN vs LSTM vs GRU for Text Generation¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "855f4c0f-f93c-4301-bd7c-56b25234cf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_RNN_model(max_sequence_len, total_words):\n",
    "    input_len = max_sequence_len - 1\n",
    "    model = Sequential()\n",
    "    \n",
    "    # add input embedding layer\n",
    "    model.add(Embedding(total_words,\n",
    "                        32,\n",
    "                        input_length=input_len))\n",
    "    \n",
    "    # input_dim: Integer. Size of the vocabulary, i.e. maximum integer index + 1.\n",
    "    # output_dim: Integer. Dimension of the dense embedding.\n",
    "    # input_length: Length of input sequences, when it is constant.\n",
    "    \n",
    "    # add hidden layer 1 - RNN layer\n",
    "    model.add(SimpleRNN(200))\n",
    "    \n",
    "    # add hidden layer 2 - dropout layer \n",
    "    model.add(Dropout(0.1))\n",
    "    \n",
    "    # add output layer \n",
    "    model.add(Dense(total_words,\n",
    "                    activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "449bad4c-5b36-443e-9759-a7eecb618bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 17, 32)            114624    \n",
      "_________________________________________________________________\n",
      "simple_rnn_5 (SimpleRNN)     (None, 200)               46600     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 3582)              719982    \n",
      "=================================================================\n",
      "Total params: 881,206\n",
      "Trainable params: 881,206\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_RNN = create_RNN_model(max_sequence_len, total_words)\n",
    "model_RNN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1ce4a49a-1a31-43f0-9e3f-9aa98c061039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_LSTM_model(max_sequence_len, total_words):\n",
    "    input_len = max_sequence_len - 1\n",
    "    model = Sequential()\n",
    "        \n",
    "    # add input embedding layer\n",
    "    model.add(Embedding(total_words,\n",
    "                        32,\n",
    "                        input_length=input_len))\n",
    "    \n",
    "    # input_dim: Integer. Size of the vocabulary, i.e. maximum integer index + 1.\n",
    "    # output_dim: Integer. Dimension of the dense embedding.\n",
    "    # input_length: Length of input sequences, when it is constant.\n",
    "    \n",
    "    # add hidden layer 1 - RNN layer\n",
    "    model.add(LSTM(200))\n",
    "    \n",
    "    # add hidden layer 2 - dropout layer \n",
    "    model.add(Dropout(0.1))\n",
    "    \n",
    "    # add output layer \n",
    "    model.add(Dense(total_words,\n",
    "                    activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "12130c63-9f6d-4ef3-a79a-8669bb1dc3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 17, 32)            114624    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 200)               186400    \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 3582)              719982    \n",
      "=================================================================\n",
      "Total params: 1,021,006\n",
      "Trainable params: 1,021,006\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_LSTM = create_LSTM_model(max_sequence_len, total_words)\n",
    "model_LSTM.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "39110cff-b04b-4399-81bc-5788dca1d635",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_GRU_model(max_sequence_len, total_words):\n",
    "    input_len = max_sequence_len - 1\n",
    "    model = Sequential()\n",
    "        \n",
    "    # add input embedding layer\n",
    "    model.add(Embedding(total_words,\n",
    "                        32,\n",
    "                        input_length=input_len))\n",
    "    \n",
    "    # input_dim: Integer. Size of the vocabulary, i.e. maximum integer index + 1.\n",
    "    # output_dim: Integer. Dimension of the dense embedding.\n",
    "    # input_length: Length of input sequences, when it is constant.\n",
    "    \n",
    "    # add hidden layer 1 - RNN layer\n",
    "    model.add(GRU(200))\n",
    "    \n",
    "    # add hidden layer 2 - dropout layer \n",
    "    model.add(Dropout(0.1))\n",
    "    \n",
    "    # add output layer \n",
    "    model.add(Dense(total_words,\n",
    "                    activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam')\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "21f731ef-b1d4-4937-a079-653d1b57310a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 17, 32)            114624    \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 200)               139800    \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 3582)              719982    \n",
      "=================================================================\n",
      "Total params: 974,406\n",
      "Trainable params: 974,406\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_GRU = create_GRU_model(max_sequence_len, total_words)\n",
    "model_GRU.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "032f4519-c87e-47d0-bd46-92679a48188b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting early_stopping feature to stop early on getting stagnat\n",
    "early_stopping = EarlyStopping(min_delta=0.01, # minimum amount of change to count as an improvement\n",
    "                               patience=5, # how many epochs to wait before stopping\n",
    "                               restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a284a4-1795-4dfb-bd06-93ec8b059b89",
   "metadata": {},
   "source": [
    "## Fit models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0aabac6-c425-4cec-824b-8cb7d1314563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/envs/nlp_course/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-01 22:23:12.113452: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2\n",
      "2023-11-01 22:23:12.114557: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 10. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8032/8057 [============================>.] - ETA: 0s - loss: 7.6192WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "8057/8057 [==============================] - 17s 2ms/sample - loss: 7.6200\n",
      "Epoch 2/200\n",
      "8032/8057 [============================>.] - ETA: 0s - loss: 7.0843WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "8057/8057 [==============================] - 16s 2ms/sample - loss: 7.0833\n",
      "Epoch 3/200\n",
      "8032/8057 [============================>.] - ETA: 0s - loss: 6.7677WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "8057/8057 [==============================] - 16s 2ms/sample - loss: 6.7690\n",
      "Epoch 4/200\n",
      "8032/8057 [============================>.] - ETA: 0s - loss: 6.3785WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "8057/8057 [==============================] - 16s 2ms/sample - loss: 6.3781\n",
      "Epoch 5/200\n",
      "8032/8057 [============================>.] - ETA: 0s - loss: 5.9388WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "8057/8057 [==============================] - 16s 2ms/sample - loss: 5.9408\n",
      "Epoch 6/200\n",
      "8032/8057 [============================>.] - ETA: 0s - loss: 5.4868WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "8057/8057 [==============================] - 16s 2ms/sample - loss: 5.4889\n",
      "Epoch 7/200\n",
      "8032/8057 [============================>.] - ETA: 0s - loss: 5.0048WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "8057/8057 [==============================] - 16s 2ms/sample - loss: 5.0059\n",
      "Epoch 8/200\n",
      "8032/8057 [============================>.] - ETA: 0s - loss: 4.4997WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "8057/8057 [==============================] - 16s 2ms/sample - loss: 4.5022\n",
      "Epoch 9/200\n",
      "8032/8057 [============================>.] - ETA: 0s - loss: 3.9952WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "8057/8057 [==============================] - 16s 2ms/sample - loss: 3.9961\n",
      "Epoch 10/200\n",
      "8032/8057 [============================>.] - ETA: 0s - loss: 3.5232WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "8057/8057 [==============================] - 16s 2ms/sample - loss: 3.5260\n",
      "Epoch 11/200\n",
      "8032/8057 [============================>.] - ETA: 0s - loss: 3.0873WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "8057/8057 [==============================] - 17s 2ms/sample - loss: 3.0889\n",
      "Epoch 12/200\n",
      "8032/8057 [============================>.] - ETA: 0s - loss: 2.7061WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "8057/8057 [==============================] - 16s 2ms/sample - loss: 2.7088\n",
      "Epoch 13/200\n",
      "8032/8057 [============================>.] - ETA: 0s - loss: 2.3718WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "8057/8057 [==============================] - 16s 2ms/sample - loss: 2.3701\n",
      "Epoch 14/200\n",
      "8032/8057 [============================>.] - ETA: 0s - loss: 2.1034WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "8057/8057 [==============================] - 16s 2ms/sample - loss: 2.1044\n",
      "Epoch 15/200\n",
      "8032/8057 [============================>.] - ETA: 0s - loss: 1.8647WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "8057/8057 [==============================] - 15s 2ms/sample - loss: 1.8677\n",
      "Epoch 16/200\n",
      "8032/8057 [============================>.] - ETA: 0s - loss: 1.6689WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "8057/8057 [==============================] - 16s 2ms/sample - loss: 1.6707\n",
      "Epoch 17/200\n",
      "8032/8057 [============================>.] - ETA: 0s - loss: 1.4841WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "8057/8057 [==============================] - 16s 2ms/sample - loss: 1.4848\n",
      "Epoch 18/200\n",
      "8032/8057 [============================>.] - ETA: 0s - loss: 1.3303WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "8057/8057 [==============================] - 16s 2ms/sample - loss: 1.3304\n",
      "Epoch 19/200\n",
      "8032/8057 [============================>.] - ETA: 0s - loss: 1.1978WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "8057/8057 [==============================] - 15s 2ms/sample - loss: 1.1964\n",
      "Epoch 20/200\n",
      "8032/8057 [============================>.] - ETA: 0s - loss: 1.0769WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "8057/8057 [==============================] - 15s 2ms/sample - loss: 1.0793\n",
      "Epoch 21/200\n",
      "8032/8057 [============================>.] - ETA: 0s - loss: 0.9759WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "8057/8057 [==============================] - 16s 2ms/sample - loss: 0.9758\n",
      "Epoch 22/200\n",
      "8032/8057 [============================>.] - ETA: 0s - loss: 0.8875WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "8057/8057 [==============================] - 16s 2ms/sample - loss: 0.8871\n",
      "Epoch 23/200\n",
      "8032/8057 [============================>.] - ETA: 0s - loss: 0.8077WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "8057/8057 [==============================] - 16s 2ms/sample - loss: 0.8093\n",
      "Epoch 24/200\n",
      "8032/8057 [============================>.] - ETA: 0s - loss: 0.7366WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "8057/8057 [==============================] - 16s 2ms/sample - loss: 0.7363\n",
      "Epoch 25/200\n",
      "8032/8057 [============================>.] - ETA: 0s - loss: 0.6787WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "8057/8057 [==============================] - 16s 2ms/sample - loss: 0.6786\n",
      "Epoch 26/200\n",
      "8032/8057 [============================>.] - ETA: 0s - loss: 0.6254WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "8057/8057 [==============================] - 16s 2ms/sample - loss: 0.6253\n",
      "Epoch 27/200\n",
      "8032/8057 [============================>.] - ETA: 0s - loss: 0.5900WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "8057/8057 [==============================] - 16s 2ms/sample - loss: 0.5897\n",
      "Epoch 28/200\n",
      "8032/8057 [============================>.] - ETA: 0s - loss: 0.5471WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "8057/8057 [==============================] - 16s 2ms/sample - loss: 0.5475\n",
      "Epoch 29/200\n",
      "8032/8057 [============================>.] - ETA: 0s - loss: 0.5161WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "8057/8057 [==============================] - 16s 2ms/sample - loss: 0.5172\n",
      "Epoch 30/200\n",
      "8032/8057 [============================>.] - ETA: 0s - loss: 0.4953WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "8057/8057 [==============================] - 16s 2ms/sample - loss: 0.4948\n",
      "Epoch 31/200\n",
      "8032/8057 [============================>.] - ETA: 0s - loss: 0.4768WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "8057/8057 [==============================] - 16s 2ms/sample - loss: 0.4769\n",
      "Epoch 32/200\n",
      "8032/8057 [============================>.] - ETA: 0s - loss: 0.4549WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "8057/8057 [==============================] - 17s 2ms/sample - loss: 0.4556\n",
      "Epoch 33/200\n",
      "8032/8057 [============================>.] - ETA: 0s - loss: 0.4474WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "8057/8057 [==============================] - 16s 2ms/sample - loss: 0.4479\n",
      "Epoch 34/200\n",
      "8032/8057 [============================>.] - ETA: 0s - loss: 0.4256WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "8057/8057 [==============================] - 16s 2ms/sample - loss: 0.4249\n",
      "Epoch 35/200\n",
      "8032/8057 [============================>.] - ETA: 0s - loss: 0.4108WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "8057/8057 [==============================] - 16s 2ms/sample - loss: 0.4109\n",
      "Epoch 36/200\n",
      "8032/8057 [============================>.] - ETA: 0s - loss: 0.4085WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "8057/8057 [==============================] - 16s 2ms/sample - loss: 0.4084\n",
      "Epoch 37/200\n",
      "8032/8057 [============================>.] - ETA: 0s - loss: 0.3934WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "8057/8057 [==============================] - 16s 2ms/sample - loss: 0.3940\n",
      "Epoch 38/200\n",
      "8032/8057 [============================>.] - ETA: 0s - loss: 0.3908WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "8057/8057 [==============================] - 16s 2ms/sample - loss: 0.3904\n",
      "Epoch 39/200\n",
      "2944/8057 [=========>....................] - ETA: 10s - loss: 0.3369"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model_RNN.fit(predictors,\n",
    "              label,\n",
    "              epochs=200,\n",
    "              callbacks=[early_stopping])\n",
    "clear_output(wait=True)\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081fd538-fb7f-4d5d-b5d0-1402a92d232f",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_time = end_time - start_time\n",
    "total_time_td = time.strftime(\"%H:%M:%S\", time.gmtime(total_time))\n",
    "print(f'Total time: {total_time_td}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326d60d3-acf7-4720-b27c-b1560c95c540",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_RNN.save('model_RNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354f3008-9866-4ab2-8696-1874ddf60109",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "model_LSTM.fit(predictors,\n",
    "              label,\n",
    "              epochs=200,\n",
    "              callbacks=[early_stopping])\n",
    "clear_output(wait=True)\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7657a56e-1f52-4e5f-867c-04cc37477469",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_time = end_time - start_time\n",
    "total_time_td = time.strftime(\"%H:%M:%S\", time.gmtime(total_time))\n",
    "print(f'Total time: {total_time_td}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae668c9-7c59-42f4-bcf9-2cafc0dcbf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LSTM.save('model_LSTM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3016322b-d6d9-4229-8445-db637b2e1087",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "model_GRU.fit(predictors,\n",
    "              label,\n",
    "              epochs=200,\n",
    "              callbacks=[early_stopping])\n",
    "clear_output(wait=True)\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287b7847-f097-4d29-8009-eaae1bfd6042",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_time = end_time - start_time\n",
    "total_time_td = time.strftime(\"%H:%M:%S\", time.gmtime(total_time))\n",
    "print(f'Total time: {total_time_td}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d7ef1b-75f8-48a6-aaaa-39ec4bafabbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_GRU.save('model_GRU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23755eaa-862a-4d92-97f6-04e562ed48e8",
   "metadata": {},
   "source": [
    "## Compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc3498a-3a8f-4ee8-b0da-7203b4c730ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_RNN = history_RNN.history['loss']\n",
    "val_loss_RNN = history_RNN.history['val_loss']\n",
    "\n",
    "loss_LSTM = history_LSTM.history['loss']\n",
    "val_loss_LSTM = history_LSTM.history['val_loss']\n",
    "\n",
    "loss_GRU = history_GRU.history['loss']\n",
    "val_loss_GRU = history_GRU.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323115f3-f450-4e2d-b878-ef20c1dc0520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = range(1, len(loss_RNN) + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(epochs, loss_RNN, 'b', label='RNN Training Loss')\n",
    "plt.plot(epochs, val_loss_RNN, 'r', label='RNN Validation Loss')\n",
    "plt.plot(epochs, loss_LSTM, 'g', label='LSTM Training Loss')\n",
    "plt.plot(epochs, val_loss_LSTM, 'y', label='LSTM Validation Loss')\n",
    "plt.plot(epochs, loss_GRU, 'c', label='GRU Training Loss')\n",
    "plt.plot(epochs, val_loss_GRU, 'm', label='GRU Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d76141-6c32-459e-bfbc-1812ed5668f2",
   "metadata": {},
   "source": [
    "## Generating the texr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b63b1e-f053-4990-bb08-948bb2123b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(seed_text, next_words, model, max_sequence_len):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], max_len]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
